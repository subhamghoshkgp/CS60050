{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2 Part 1 - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, you will implement a decision tree model.\n",
    "\n",
    "*For Even Roll Number Students:*\n",
    "\n",
    "* In this part, you have to implement a decision tree model to predict the cardio vascular disease based on various input features.\n",
    "* Noiseless Dataset: ````cardio.csv````\n",
    "* Noisy Dataset: ````cardio_noise.csv````\n",
    "\n",
    "*For Odd Roll Number Students:*\n",
    "\n",
    "* In this part, you have to implement a decision tree model to predict whether a patient has diabetes based on various input features.\n",
    "* Noiseless Dataset: ````diabetes.csv````\n",
    "* Noisy Dataset: ````diabetes_noise.csv````\n",
    "\n",
    "The assignment zip file (ML_CS60050_A2.zip) contains the respective datasets which will be used in this assignment.\n",
    "\n",
    "You have to write your code in this jupyter notebook. You have to write your code only between ### START CODE HERE ### and ### END CODE HERE ### comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Submission Instructions\n",
    "\n",
    "Please submit your assignment as a ZIP file that contains a folder named in the following format: `RollNo_ML_A2`. Inside this folder, include two Jupyter notebooks and a Report with the following names:\n",
    "\n",
    "1. `RollNo_A2_Part1.ipynb`\n",
    "2. `RollNo_A2_Part2.ipynb`\n",
    "3. `RollNo_report.pdf`\n",
    "\n",
    "\n",
    "Instructions for the Report:\n",
    "* Summarize results from noiseless and noisy datasets.\n",
    "* Compare performance and note the impact of noise.\n",
    "* Conclude with key findings andÂ implications.\n",
    "\n",
    "Make sure that you replace `RollNo` with your actual roll number in both the folder name and the notebook filenames.\n",
    "\n",
    "For example, if your roll number is `23CS60R11`, the folder should be named `23CS60R11_ML_A2`, and the three files should be named `23CS60R11_A2_Part1.ipynb`, `23CS60R11_A2_Part2.ipynb` and `RollNo_report.pdf`.\n",
    "\n",
    "Submit this ZIP file as your assignment submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas, numpy, seaborn, and matplotlib\n",
    "! pip install pandas numpy seaborn matplotlib\n",
    "\n",
    "# Install scikit-learn\n",
    "! pip install scikit-learn\n",
    "\n",
    "# Install IPython\n",
    "! pip install ipython\n",
    "\n",
    "# Install pygraphviz\n",
    "! pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions given in the file ````graphviz_installation.txt```` to install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from graphviz import Digraph\n",
    "from IPython.display import Image, display\n",
    "import pygraphviz as pgv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\ML_CS60050_A2\\ML_CS60050_A2\\cardio.csv\") # Replace with noise/noiseless dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train, Validation, Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(df,test_size=0.2,random_state=1)\n",
    "train_df,val_df = train_test_split(train_df,test_size=0.2,random_state=1)\n",
    "train_df.shape,test_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the algorithm works\n",
    "\n",
    "**We'll start with all examples at the root node then:**\n",
    "\n",
    "**We'll calculate information gain for splitting on all possible features and pick the one with the highest value**\n",
    "\n",
    "**Then we'll split the data according to the selected feature**\n",
    "\n",
    "**We'll repeat this  process until stopping criteria is met**\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "### Entropy\n",
    "**Entropy function which is a way to measure impurity**\n",
    "\n",
    "**Entropy is represented by this function**\n",
    "$$H = -\\sum\\limits_{}^{} p_{i}\\text{log}_2 p_{i} \n",
    "$$\n",
    "\n",
    "**Where $(p_1)$ is the fraction of examples that are a certain class**\n",
    "\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "**Information gain is the reduction in entropy when he make a split**\n",
    "\n",
    "**Recall that our goal is to choose the split that gives the highest information gain, information gain equation =**\n",
    "\n",
    "**$$H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$**\n",
    "**where** \n",
    "- $H(p_1^\\text{node})$ is entropy at the node \n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree visualization using graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pgv.AGraph(strict=True, directed=True)\n",
    "graph2 = pgv.AGraph(strict=True, directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygraphviz as pgv\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = Node(None)\n",
    "        self.height = -1\n",
    "\n",
    "    def find_splits(self, data, column_index):\n",
    "        \"\"\"\n",
    "        Identifies potential split points for a given feature.\n",
    "        \n",
    "        Parameters:\n",
    "            data (numpy.ndarray): The dataset used for finding splits.\n",
    "            column_index (int): The index of the column for which to find potential splits.\n",
    "        \n",
    "        Returns:\n",
    "            potential_splits (list): A list of potential split points for the specified feature.\n",
    "        \"\"\"\n",
    "\n",
    "        x = data[:,:-1]\n",
    "        potential_splits = []\n",
    "        \n",
    "        # Find unique values in the column\n",
    "        unique_values = np.unique(x[:, column_index])\n",
    "\n",
    "        # Calculate potential split points as the midpoints between unique values\n",
    "        for i in range(len(unique_values) - 1):\n",
    "            potential_splits.append((unique_values[i] + unique_values[i + 1]) / 2)\n",
    "        \n",
    "        return potential_splits\n",
    "\n",
    "    def calculate_entropy(self, data):\n",
    "        \"\"\"\n",
    "        Calculates the entropy of the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            data (numpy.ndarray): The dataset for which to calculate entropy.\n",
    "        \n",
    "        Returns:\n",
    "            entropy (float): The entropy value of the dataset.\n",
    "        \"\"\"\n",
    "        y = data[:, -1]\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))  # Calculate entropy using probabilities\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, data, column_no, value):\n",
    "        \"\"\"\n",
    "        Calculates the information gain resulting from splitting the data on a specific feature at a specific value.\n",
    "        \n",
    "        Parameters:\n",
    "            data (numpy.ndarray): The dataset to split.\n",
    "            column_no (int): The index of the feature used to split the data.\n",
    "            value (float): The value at which to split the feature.\n",
    "        \n",
    "        Returns:\n",
    "            information_gain (float): The information gain from the split.\n",
    "        \"\"\"\n",
    "        parent_entropy = self.calculate_entropy(data)\n",
    "        left_data = data[data[:, column_no] <= value]\n",
    "        right_data = data[data[:, column_no] > value]\n",
    "\n",
    "        n = len(left_data) + len(right_data)\n",
    "        p_left_data = len(left_data) / n\n",
    "        p_right_data = len(right_data) / n\n",
    "\n",
    "        child_entropy = p_left_data * self.calculate_entropy(left_data) + p_right_data * self.calculate_entropy(right_data)\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def majority(self, data):\n",
    "        \"\"\"\n",
    "        Determines the majority class label in the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            data (pandas.DataFrame): The dataset to classify.\n",
    "        \n",
    "        Returns:\n",
    "            majority_class (int): The label of the majority class.\n",
    "        \"\"\"\n",
    "        cls, count = np.unique(data[\"cardio\"], return_counts=True) \n",
    "        return cls[np.argmax(count)]\n",
    "\n",
    "    def classify(self, data, edge):\n",
    "        \"\"\"\n",
    "        Classifies a dataset as a leaf node.\n",
    "        \n",
    "        Parameters:\n",
    "            data (pandas.DataFrame): The dataset to classify.\n",
    "            edge (int): The index of the parent edge.\n",
    "        \n",
    "        Returns:\n",
    "            leaf (Node): A leaf node with classification information.\n",
    "        \"\"\"\n",
    "        classification = self.majority(data)\n",
    "        entropy = self.calculate_entropy(data.values)\n",
    "        d = {\"ID\": \"Leaf\", \"Classification\": classification, \"Parent_Edge\": edge, \"Entropy\": entropy, \"Samples\": data.shape[0]}\n",
    "        leaf = Node(d)\n",
    "        return leaf\n",
    "    \n",
    "\n",
    "    def build_tree(self, data, max_depth, attributes, edge, height):\n",
    "        \"\"\"\n",
    "        A recursive utility function for building the decision tree.\n",
    "        \n",
    "        Parameters:\n",
    "            data (pandas.DataFrame): The dataset to build the tree from.\n",
    "            max_depth (int): The maximum allowed depth of the tree.\n",
    "            attributes (list): The list of attributes used in the dataset.\n",
    "            edge (int): The index of the parent edge.\n",
    "            height (int): The current height of the tree.\n",
    "        \n",
    "        Returns:\n",
    "            node (Node): The root node of the subtree created.\n",
    "        \"\"\"\n",
    "\n",
    "        if height == max_depth or len(np.unique(data[\"cardio\"])) == 1:  # Check for termination conditions\n",
    "            return self.classify(data, edge)  # Create a leaf node\n",
    "\n",
    "        best = {\"ID\": \"\", \"best_attribute\": \"\", \"best_gain\": -1, \"best_split\": -1, \"best_entropy\": -1}\n",
    "\n",
    "        # Find the best attribute to split on\n",
    "        for i in range(len(attributes)):\n",
    "            potential_splits = self.find_splits(data.values, i)\n",
    "            for value in potential_splits:\n",
    "                information_gain = self.calculate_information_gain(data.values, i, value)\n",
    "                if information_gain > best[\"best_gain\"]:  # Update best only if gain is positive\n",
    "                    best[\"best_attribute\"] = attributes[i]\n",
    "                    best[\"best_gain\"] = information_gain\n",
    "                    best[\"best_split\"] = value\n",
    "                    best[\"best_entropy\"] = self.calculate_entropy(data.values)\n",
    "\n",
    "        # If no positive information gain is found, create a leaf node\n",
    "        if best[\"best_gain\"] == -1:\n",
    "            return self.classify(data, edge)\n",
    "\n",
    "        _, sample = np.unique(data[\"cardio\"], return_counts=True)\n",
    "        d = {\"ID\": best[\"best_attribute\"], \"Entropy\": best[\"best_entropy\"], \n",
    "             \"Samples\": data.shape[0], \"Parent_Edge\": edge, \n",
    "             \"Best_Split\": best[\"best_split\"], \"Values\": sample}\n",
    "        node = Node(d)\n",
    "        node.left = self.build_tree(data[data[best[\"best_attribute\"]] <= best[\"best_split\"]], max_depth, attributes, 2 * edge + 1, height + 1)\n",
    "        node.right = self.build_tree(data[data[best[\"best_attribute\"]] > best[\"best_split\"]], max_depth, attributes, 2 * edge + 2, height + 1)\n",
    "\n",
    "        root = f'{d[\"ID\"]} <= {d[\"Best_Split\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nValues = {sample}'\n",
    "        \n",
    "        graph.add_node(str(edge), label=root)\n",
    "        graph.add_edge(str(edge), str(2 * edge + 1))\n",
    "        graph.add_edge(str(edge), str(2 * edge + 2))\n",
    "        return node\n",
    "\n",
    "    def fit(self, data, max_depth=100):\n",
    "        \"\"\"\n",
    "        Fits the decision tree model to the provided dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            data (pandas.DataFrame): The dataset to fit the tree to.\n",
    "            max_depth (int): The maximum allowed depth of the tree.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        attributes = data.columns.tolist()[:-1]\n",
    "        self.attributes = np.array(attributes)\n",
    "        self.root = self.build_tree(data, max_depth, attributes, 0, 0)\n",
    "        \n",
    "    def plt(self, graph, node, vert):\n",
    "        \"\"\"\n",
    "        Plots the decision tree using a graph representation.\n",
    "        \n",
    "        Parameters:\n",
    "            graph (Graph): The graph object to use for plotting.\n",
    "            node (Node): The current node in the tree.\n",
    "            vert (int): The current vertex in the graph.\n",
    "        \n",
    "        Returns:\n",
    "            root (str): The label of the current node.\n",
    "        \"\"\"\n",
    "        d = node.data\n",
    "        if \"Classification\" in node.data.keys():\n",
    "            root = f'{d[\"ID\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nClass = {d[\"Classification\"]}'\n",
    "            graph.add_node(str(vert), label=root)\n",
    "            return root\n",
    "        \n",
    "        root = f'{d[\"ID\"]} <= {d[\"Best_Split\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nValues = {d[\"Values\"]}'\n",
    "        graph.add_node(str(vert), label=root)\n",
    "        root1 = self.plt(graph, node.left, 2 * vert + 1)\n",
    "        graph.add_node(str(2 * vert + 1), label=root1)\n",
    "        root2 = self.plt(graph, node.right, 2 * vert + 2)\n",
    "        graph.add_node(str(2 * vert + 2), label=root2)\n",
    "        \n",
    "        graph.add_edge(str(vert), str(2 * vert + 1))\n",
    "        graph.add_edge(str(vert), str(2 * vert + 2))\n",
    "        return root\n",
    "\n",
    "    def prune_util(self, node):\n",
    "        \"\"\"\n",
    "        Utility function for pruning a node in the decision tree.\n",
    "        \n",
    "        Parameters:\n",
    "            node (Node): The node to prune.\n",
    "        \n",
    "        Returns:\n",
    "            d (dict): The data for the pruned leaf node.\n",
    "        \"\"\"\n",
    "        d = {\"ID\": \"Leaf\", \"Classification\": 0, \"Parent_Edge\": node.data[\"Parent_Edge\"], \n",
    "             \"Entropy\": 0, \"Samples\": node.data[\"Samples\"]}\n",
    "        if node.data[\"Values\"][0] > node.data[\"Values\"][1]:\n",
    "            d[\"Classification\"] = 0\n",
    "        else:\n",
    "            d[\"Classification\"] = 1\n",
    "        return d\n",
    "        \n",
    "    def prune(self, val_df, node):\n",
    "        \"\"\"\n",
    "        Prunes the decision tree to avoid overfitting.\n",
    "        \n",
    "        Parameters:\n",
    "            val_df (pandas.DataFrame): The validation dataset used to evaluate pruning.\n",
    "            node (Node): The current node to consider pruning.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if \"Classification\" in node.data.keys():\n",
    "            return\n",
    "\n",
    "        self.prune(val_df, node.left)\n",
    "        self.prune(val_df, node.right)\n",
    "\n",
    "        curr_val = accuracy_score(val_df.values[:, -1], self.predict(val_df.values[:, :-1]))\n",
    "\n",
    "        tmp1 = node.left\n",
    "        tmp2 = node.right\n",
    "        tmp = node.data\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "\n",
    "        node.data = self.prune_util(node)\n",
    "\n",
    "        new_val = accuracy_score(val_df.values[:, -1], self.predict(val_df.values[:, :-1]))\n",
    "\n",
    "        if new_val < curr_val:  # Revert pruning if accuracy decreases\n",
    "            node.left = tmp1\n",
    "            node.right = tmp2\n",
    "            node.data = tmp\n",
    "\n",
    "    def predict_one(self, data):\n",
    "        \"\"\"\n",
    "        Predicts the class label for a single data point.\n",
    "        \n",
    "        Parameters:\n",
    "            data (numpy.ndarray): The data point to classify.\n",
    "        \n",
    "        Returns:\n",
    "            classification (int): The predicted class label.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        while \"Classification\" not in node.data.keys():\n",
    "            d = node.data\n",
    "            if data[np.argwhere(self.attributes == d[\"ID\"]).squeeze()] <= d[\"Best_Split\"]:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.data[\"Classification\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for a dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            X (numpy.ndarray): The dataset to classify.\n",
    "        \n",
    "        Returns:\n",
    "            Y_pred (numpy.ndarray): The predicted class labels.\n",
    "        \"\"\"\n",
    "        Y_pred = []\n",
    "        for i in range(X.shape[0]):\n",
    "            Y_pred.append(self.predict_one(X[i, :]))\n",
    "        return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTree()\n",
    "model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Decision Tree before Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.layout(prog=\"neato\") \n",
    "graph.draw(\"graph.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='Decision_Tree_Before_Pruning.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing before Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.values[:,:-1]\n",
    "Y_test = test_df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy,Macro Precision, Macro Recall Before Pruning\")\n",
    "accuracy_score(Y_test,Y_pred),precision_score(Y_test,Y_pred,average='macro'),recall_score(Y_test,Y_pred,average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Error Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prune(val_df,model.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ =model.plt(graph2,model.root,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2.draw(\"Decision_Tree_After_Pruning.png\", prog=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='Decision_Tree_After_Pruning.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing after Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy,Macro Precision, Macro Recall After Pruning\")\n",
    "accuracy_score(Y_test,Y_pred),precision_score(Y_test,Y_pred,average='macro'),recall_score(Y_test,Y_pred,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1829286,
     "sourceId": 2984728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30260,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
